<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="BARET : Balanced Attention based Real image Editing driven by Target-text Inversion">
<!--  <meta name="keywords" content="PromptStyler, Prompt, Style Generation, Domain Generalization, DG, Source-free Domain Generalization, Source-free DG, CLIP, Domain Adaptation, DA">-->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BARET</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">BARET : Balanced Attention based Real image Editing  <br> driven by Target-text Inversion</h1>
          <p class="is-size-3"; style="color:#808080; margin-top:-25px"> AAAI 2024 </p>
          <!-- <br> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">Yuming Qiao<sup>1,2</sup>,</span>
            <span class="author-block">Fanyi Wang<sup>1</sup>,</span>
            <span class="author-block">Jingwen Su<sup>1</sup>,</span>
            <span class="author-block">Yanhao Zhang<sup>1</sup>,</span>
            <span class="author-block">Yunjie Yu<sup>1</sup>,</span>
            <span class="author-block">Siyu Wu<sup>3</sup>,</span>
            <span class="author-block">Guo-Jun Qi<sup>1,4</sup>,</span>
<!--            <span class="author-block">-->
<!--              <a href="https://jhcho99.github.io/">Yuming Qiao</a><sup>1</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.com/citations?user=7PSBLtIAAAAJ&hl=en">Gilhyun Nam</a><sup>1</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="http://cvlab.postech.ac.kr/~sungyeon/">Sungyeon Kim</a><sup>2</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.co.kr/citations?user=mDxJj2AAAAAJ&hl=en">Hunmin Yang</a><sup>1,3</sup>,</span>-->
<!--              <span class="author-block">-->
<!--              <a href="https://suhakwak.github.io/">Suha Kwak</a><sup>2</sup>-->
<!--            </span>-->
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>OPPO Research Institute,</span>
            <span class="author-block"><sup>2</sup>Tsinghua University,</span>
            <span class="author-block"><sup>3</sup>Zhejiang University,</span>
            <span class="author-block"><sup>1</sup>Westlake University</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.05482"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.05482"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
<!--              <span class="link-block">-->
<!--                <a href="#BibTeX"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-obp"></i>-->
<!--                  </span>-->
<!--                  <span>BibTex</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              <span class="link-block">-->
<!--                <a href="mailto:PromptStyler.official@gmail.com"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-envelope"></i>-->
<!--                  </span>-->
<!--                  <span>Contact</span>-->
<!--                </a>-->
<!--              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser" style="margin-top:-40px">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/first.png"  class="center"/>
<!--      <embed src="\(pageContext.request.contextPath}/admin/indexs/images/resume.pdf" type="application/pdf"  width="800px" height="2400px">-->
<!--          <iframe src="static/images/FigureFirstshow.pdf" width="100%" height="600px"></iframe>-->

    </div>
<!--    <h2 class="subtitle has-text-centered" style="margin-top:-12px">-->
<!--      By leveraging a joint vision-language space, PromptStyler simulates various distribution shifts via learnable words for pseudo-words <b>S</b><sub>*</sub> without using any images to deal with source-free domain generalization.-->
<!--      <br><br>-->
<!--      <div class="gray-box-custom" style="margin-top:-12px">-->
<!--      Thanks to the <a href="https://arxiv.org/abs/2302.04269">cross-modal transferability phenomenon of this joint space</a>, we could train a classifier using text features while running an inference with the classifier using image features.-->
<!--      </div>-->
<!--    </h2>-->
  </div>
</section>



<!-- Abstract. -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top:-5px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Image editing approaches with diffusion models have been rapidly developed, yet their applicability are subject to requirements such as specific editing types (e.g., foreground or background object editing, style transfer), multiple conditions (e.g., mask, sketch, caption), and time consuming fine-tuning of diffusion models. For alleviating these limitations and realizing efficient real image editing, we propose a novel editing technique that only requires an input image and target text for various editing types including non-rigid edits without fine-tuning diffusion model. Our method contains three novelties:
            (I) Target-text Inversion Schedule (TTIS) is designed to fine-tune the input target text embedding to achieve fast image reconstruction without image caption and acceleration of convergence.
            (II) Progressive Transition Scheme applies progressive linear interpolation between target text embedding and its fine-tuned version to generate transition embedding for maintaining non-rigid editing capability.
            (III) Balanced Attention Module (BAM) balances the tradeoff between textual description and image semantics.
            By the means of combining self-attention map from reconstruction process and cross-attention map from transition process, the guidance of target text embeddings in diffusion process is optimized.
            In order to demonstrate editing capability, effectiveness and efficiency of the proposed BARET, we have conducted extensive qualitative and quantitative experiments. Moreover, results derived from user study and ablation study further prove the superiority over other methods.
          </p>
<!--          <p>-->
<!--              (II) Progressive Transition Scheme applies progressive linear interpolation between target text embedding and its fine-tuned version to generate transition embedding for maintaining non-rigid editing capability.          </p>-->
<!--          <p>-->
<!--            PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.-->
<!--          </p>-->
        </div>
      </div>
    </div>
    </div>
</section>
<br>



<!-- Motivation. -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top:-5px">Method</h2>
      </div>
    </div>
</section>
<br>
<section class="hero teaser" style="margin-top:-5px">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/method_baret.png" class="center-img"/>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified" style="margin-top:-22px">
          <p>
            <b>Figure 1.</b> (a) Illustration of our Target-text Inversion Schedule and Progressive transition scheme. (b)Illustration of Balanced Attention Module.
          </p>
          <p style="margin-top:40px">
            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
            Target-text inversion Schedule first applies DDIM inversion to invert original image \({I}_{org}\) from \({Z}_{0}\sim{Z}_{T}\). Then denoising \({Z}_{T}\) combined with target text condition under guidance scale=7.5. And set intermediate result as \(\{{Z}_{t}^{*}\}_{t=0}^{T-1}\). Based on reconstruction loss between \(\{{Z}_{t}^{*}\}_{t=0}^{T-1}\) and \(\{{Z}_{t}\}_{t=0}^{T-1}\), Tuning target text embedding \({\phi}_{cond}\) to \(\{{\phi}_{opt,t}\}_{t=1}^{T}\) within each timestamp to approximately reconstruct original image \({I}_{rct}\). Progressive linear transition was then adopted between target text embedding \({\phi}_{cond}\) and fine-tuned embedding \(\{{\phi}_{opt,t}\}_{t=1}^{T}\) with interpolation parameter \(\{{\omega}_{t}\}_{t=1}^{T}\) to generate interpolated embedding \(\{{\phi}_{inp,t}\}_{t=1}^{T}\). And transition result \({I}_{inp}\) output by DDIM process with \({Z}_{T}\) and \(\{{\phi}_{inp,t}\}_{t=1}^{T}\) as input will show a better integration of original image and desired non-rigid changes.
          </p>
          <p>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
            Self-attention map \({Q}_{rct}{*}{K}_{rct}^{T}\) can help to preserve the original image characteristics. Cross-attention map \({Q}_{inp}{*}{K}_{{\phi}_{inp}}^{T}\) represents the non-rigid image semantic information, which can optimize the guidance efficiency of target text by dot production with \({V}_{{\phi}_{cond}}\) and \({K}_{rct}\) from reconstruction process latent features and injecting them to editing process. In cross-attention module, getting projected \({Q}_{inp}\) and \({K}_{{\phi}_{inp}}\) from latent features and transition embedding \(\{{\phi}_{inp,t}\}_{t=1}^{T}\) in transition process. Then injecting them to editing process and dot with \({V}_{{\phi}_{cond}}\) from target text embedding \({\phi}_{cond}\).
          </p>
        </div>
      </div>
    </div>
</section>
<br>

<!-- Method Comparison. -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top:-5px">Method Comparison</h2>
      </div>
    </div>
</section>
<br>
<section class="hero teaser" style="margin-top:-5px">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/compare1.png" class="center-img"/>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified" style="margin-top:-22px">
          <p>
              <b>Figure 2.</b>We compare SDEdite, Pix2Pix-Zero, MasaCtrl, Null-text inversion, Imagic and SINE to our method. Our BARET can be applied in arbitrary edits (e.g., posture changes, style transfer, background/object editing, and object addition), while preserving characteristics of original image.
          </p>
        </div>
      </div>
    </div>
</section>
<br>


<hr>
<!--<br>-->
<!-- Paper. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top:-5px">paper</h2>
      </div>
    </div>
</section>
<br>

<br>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
<!--     <style>-->
<!--    table {-->
<!--      position: relative;-->
<!--      /*top: 0px;*/-->
<!--      left: 0px;-->
<!--      right: 1000px;-->
<!--    }-->
<!--</style>-->
    <table>
      <tr>
      <td valign="top">
        <img src="static/images/paper.png" style="width:300px;" />
      </td>
      <td valign="left">
        <b>"BARET : Balanced Attention based Real image Editing driven by Target-text Inversion"</b><br />
        Yuming Qiao, Fanyi Wang, Jingwen Su, Yanhao Zhang, Yunjie Yu, Siyu Wu, Guo-Jun Qi.<br />
        AAAI Conference on Artificial Intelligence (AAAI) 2024<br />
        <b>[</b><a href="https://arxiv.org/abs/2312.05482.pdf" class="lnk">PDF</a><b>]</b>
      </td>
      </tr>
    </table>
        </div>
    </div>
    </div>
</section>
<br>


<!-- Contact.-->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top:-5px">Contact</h2>
        <div class="is-centered has-text-centered is-size-5">
          <p>
            BARET (<a href="mailto:qym21@mails.tsinghua.edu.cn">qym21@mails.tsinghua.edu.cn</a>)
          </p>
        </div>
      </div>
    </div>
</section>

<hr>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>@misc{qiao2023baret,
      title={BARET : Balanced Attention based Real image Editing driven by Target-text Inversion},
      author={Yuming Qiao and Fanyi Wang and Jingwen Su and Yanhao Zhang and Yunjie Yu and Siyu Wu and Guo-Jun Qi},
      year={2023},
      eprint={2312.05482},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>
<br><br>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2312.05482.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
<!--      <a class="icon-link" href="https://jhcho99.github.io/" class="external-link" disabled>-->
<!--        <i class="fas fa-user"></i>-->
<!--      </a>-->
      <a class="icon-link" href="mailto:qym21@mails.tsinghua.edu.cn" class="external-link" disabled>
        <i class="fas fa-envelope"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website template was adapted from <a href="https://promptstyler.github.io/">promptstyler website</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>


<!--    <div id="bibtex" style="border-top:1px solid gray;">-->
<!--    <h2>BibTeX</h2>-->
<!--&lt;!&ndash;    <center><pre><code>@inproceedings{kawar2023imagic,&ndash;&gt;-->
<!--&lt;!&ndash;      title={Imagic: Text-Based Real Image Editing with Diffusion Models},&ndash;&gt;-->
<!--&lt;!&ndash;      author={Kawar, Bahjat and Zada, Shiran and Lang, Oran and Tov, Omer and Chang, Huiwen and Dekel, Tali and Mosseri, Inbar and Irani, Michal},&ndash;&gt;-->
<!--&lt;!&ndash;      booktitle={Conference on Computer Vision and Pattern Recognition 2023},&ndash;&gt;-->
<!--&lt;!&ndash;      year={2023}&ndash;&gt;-->
<!--&lt;!&ndash;}</code></pre></center>&ndash;&gt;-->
<!--    </div>-->

<!--&lt;!&ndash; PromptStyler. &ndash;&gt;-->
<!--<hr>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3" style="margin-top:-5px">PromptStyler</h2>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->
<!--<section class="hero teaser" style="margin-top:-5px">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <img src="static/images/Factors.png" class="center-img"/>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <div class="content has-text-justified" style="margin-top:-22px">-->
<!--          <p>-->
<!--            <b>Figure 2.</b> Important factors in the proposed method. PromptStyler learns style word vectors for pseudo-words <b>S</b><sub>*</sub> which lead to diverse style features (from "a <b>S</b><sub>*</sub> style of a") while preserving content information encoded in style-content features (from "a <b>S</b><sub>*</sub> style of a [class]"). Our method maximizes <i>style diversity</i> and <i>content consistency</i> in a hyperspherical joint vision-language space (e.g., CLIP latent space).-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->
<!--<section class="hero teaser" style="margin-top:20px">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <img src="static/images/Overview.png" class="center-img"/>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <div class="content has-text-justified" style="margin-top:-22px">-->
<!--          <p>-->
<!--            <b>Figure 3.</b> PromptStyler learns diverse style word vectors which do not distort content information of style-content prompts. After learning style word vectors, we synthesize style-content features (e.g., from "a <b>S</b><sub><b>1</b></sub> style of a <b>dog</b>") via a pre-trained text encoder for training a linear classifier. The classifier is trained by a classification loss using those synthesized features and their corresponding class labels (e.g., "<b>dog</b>"). At inference time, a pre-trained image encoder extracts image features, which are fed as input to the trained classifier. Note that the encoders are derived from the same vision-language model (e.g., CLIP).-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->



<!-- Experimental Results. -->
<!--<hr>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3" style="margin-top:-5px">Experimental Results</h2>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->
<!--<section class="hero teaser" style="margin-top:-5px">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <img src="static/images/Table2.png" class="center-img"/>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <div class="content has-text-justified" style="margin-top:-22px">-->
<!--          <p>-->
<!--            <b>Table 2.</b> Comparison with the state-of-the-art domain generalization methods. ZS-CLIP (C) denotes zero-shot CLIP using "[class]" as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using "a photo of a [class]" as its text prompt. Note that PromptStyler does not exploit any source domain data and domain descriptions.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->
<!--<section class="hero teaser" style="margin-top:20px">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <img src="static/images/Figure4.png" class="center-img"/>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <div class="content has-text-justified" style="margin-top:-22px">-->
<!--          <p>-->
<!--            <b>Figure 4.</b> t-SNE visualization results for the target task VLCS (5 classes) using synthesized style-content features. We visualize such features obtained from the learned 80 style word vectors and all the 5 classes (bird, car, chair, dog, person). Different colors denote features obtained from different style word vectors, and different shapes indicate features obtained from different class names. We only colorize features from the first 10 styles. Combining the style diversity loss and content consistency loss leads to diverse styles while preserving content information.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->
<!--<section class="hero teaser" style="margin-top:20px">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <img src="static/images/Figure6.png" class="center-img"/>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <div class="content has-text-justified" style="margin-top:-22px">-->
<!--          <p>-->
<!--            <b>Figure 6.</b> Top-1 classification accuracy on the PACS, VLCS, OfficeHome and DomainNet datasets with regard to the number of learnable style word vectors.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->
<!--<section class="hero teaser" style="margin-top:20px">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <img src="static/images/Figure7.png" class="center-img"/>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <div class="content has-text-justified" style="margin-top:-22px">-->
<!--          <p>-->
<!--            <b>Figure 7.</b> Top-1 classification accuracy on the PACS, VLCS, OfficeHome and DomainNet datasets with regard to the number of training iterations for learning each style word vector.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--<br>-->




